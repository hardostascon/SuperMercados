# üõí SuperMercados ‚Äî Price Scraper

> Sistema automatizado de scraping, procesamiento y an√°lisis de precios de supermercados colombianos.

---

## üìã Descripci√≥n

**SuperMercados** es una plataforma de extracci√≥n y an√°lisis de precios que permite rastrear, comparar y almacenar los precios de productos de m√∫ltiples cadenas de supermercados en Colombia. El sistema utiliza **Scrapy** para el scraping, **Celery** para la orquestaci√≥n de tareas as√≠ncronas y **Docker** para el despliegue del entorno.

Los supermercados actualmente soportados incluyen:

- üè™ Carulla
- üè™ Mercar
- üè™ Surtifamiliar
- üè™ Ara
- üè™ Canaveral
- üè™ √âxito

---

## üèóÔ∏è Arquitectura del Proyecto

```
SuperMercados/
‚îú‚îÄ‚îÄ app/                        # API REST con FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ routers/                # Endpoints organizados por dominio
‚îÇ   ‚îú‚îÄ‚îÄ models/                 # Modelos Pydantic / ORM
‚îÇ   ‚îú‚îÄ‚îÄ database.py             # Conexi√≥n a base de datos
‚îÇ   ‚îî‚îÄ‚îÄ main.py                 # Instancia FastAPI y registro de routers
‚îú‚îÄ‚îÄ Depurador_datos/            # M√≥dulo de limpieza y normalizaci√≥n de datos
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ src/
‚îú‚îÄ‚îÄ scrappers/                  # M√≥dulo de scraping (Scrapy)
‚îÇ   ‚îú‚îÄ‚îÄ precio_scrapers/        # Proyecto Scrapy principal
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ spiders/            # Spiders por supermercado
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ middlewares.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipelines.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ settings.py
‚îÇ   ‚îî‚îÄ‚îÄ workers/                # Tareas as√≠ncronas con Celery
‚îÇ       ‚îú‚îÄ‚îÄ celery_app.py
‚îÇ       ‚îî‚îÄ‚îÄ tasks.py
‚îú‚îÄ‚îÄ tests/                      # Suite de pruebas
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ main.py
‚îî‚îÄ‚îÄ requirements.txt
```

---

## ‚öôÔ∏è Tecnolog√≠as Utilizadas

| Tecnolog√≠a | Uso |
|---|---|
| **Python 3.x** | Lenguaje principal |
| **Scrapy** | Framework de scraping |
| **FastAPI** | API REST para consulta de precios y gesti√≥n de scrapers |
| **Celery** | Orquestaci√≥n de tareas as√≠ncronas |
| **Docker / Docker Compose** | Contenedorizaci√≥n del entorno |
| **pytest** | Framework de pruebas |

---

## üöÄ Instalaci√≥n y Configuraci√≥n

### Prerrequisitos

- Python 3.9+
- Docker y Docker Compose
- Redis (broker para Celery)

### 1. Clonar el repositorio

```bash
git clone https://github.com/hardostascon/SuperMercados.git
cd SuperMercados
```

### 2. Configurar variables de entorno

Copia el archivo `.env` de ejemplo y configura tus variables:

```bash
cp .env .env.local
```

Edita el archivo `.env` con tus credenciales de base de datos y configuraci√≥n de broker.

### 3. Opci√≥n A ‚Äî Instalaci√≥n con Docker (recomendado)

```bash
docker-compose up --build
```

### 3. Opci√≥n B ‚Äî Instalaci√≥n local

```bash
# Crear y activar entorno virtual
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
```

---

## üï∑Ô∏è Uso del Scraper

### Ejecutar un spider espec√≠fico

```bash
cd scrappers
scrapy crawl carulla_spider
scrapy crawl mercar_spider
scrapy crawl surtifamiliar_spider
```

### Ejecutar el sistema completo v√≠a main

```bash
python main.py
```

### Ejecutar con Celery (modo as√≠ncrono)

```bash
# Iniciar el worker de Celery
celery -A workers.celery_app worker --loglevel=info

# Disparar tareas
python -c "from workers.tasks import run_scrapers; run_scrapers.delay()"
```

---

## üß™ Pruebas

```bash
pytest
```

Para ejecutar un m√≥dulo de pruebas espec√≠fico:

```bash
pytest tests/test_carulla.py
pytest tests/test_mercar.py
pytest tests/test_db.py
pytest tests/test_surtifamiliar.py
```

---

## üîÑ Pipeline de Datos

El flujo de datos del sistema sigue estos pasos:

```
Spider (scraping) ‚Üí Pipeline (validaci√≥n) ‚Üí Depurador_datos (limpieza) ‚Üí Base de datos ‚Üê FastAPI (consulta)
                                                                                          ‚Üë
                                                                                    Celery (tareas)
```

1. **Scraping:** Los spiders extraen los datos de precios de cada supermercado.
2. **Validaci√≥n:** El pipeline de Scrapy filtra y valida los √≠tems extra√≠dos.
3. **Limpieza:** El m√≥dulo `Depurador_datos` normaliza los datos (unidades, nombres, precios).
4. **Almacenamiento:** Los datos procesados se persisten en la base de datos o se exportan a JSON.

---

## üåê API REST con FastAPI

El m√≥dulo `app/` expone una API REST construida con **FastAPI** que permite consultar los precios scrapeados y disparar scrapers bajo demanda.

### Levantar la API

```bash
# Modo desarrollo (con auto-reload)
uvicorn app.main:app --reload --port 8000

# Modo producci√≥n
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

### Documentaci√≥n interactiva

Una vez levantada la API, FastAPI genera autom√°ticamente la documentaci√≥n en:

| Interfaz | URL |
|---|---|
| **Swagger UI** | `http://localhost:8000/docs` |
| **ReDoc** | `http://localhost:8000/redoc` |

### Endpoints principales

#### üì¶ Precios

| M√©todo | Endpoint | Descripci√≥n |
|---|---|---|
| `GET` | `/productos` | Lista todos los productos con sus precios |
| `GET` | `/productos/{id}` | Consulta un producto espec√≠fico |
| `GET` | `/productos?supermercado=carulla` | Filtra productos por supermercado |
| `GET` | `/productos?nombre=leche` | Busca productos por nombre |

#### üï∑Ô∏è Scrapers

| M√©todo | Endpoint | Descripci√≥n |
|---|---|---|
| `POST` | `/scrapers/run` | Dispara todos los scrapers |
| `POST` | `/scrapers/run/{supermercado}` | Dispara el scraper de un supermercado |
| `GET` | `/scrapers/status` | Consulta el estado de los scrapers en ejecuci√≥n |

#### üîê Autenticaci√≥n

| M√©todo | Endpoint | Descripci√≥n |
|---|---|---|
| `POST` | `/auth/login` | Obtiene un token JWT |
| `POST` | `/auth/refresh` | Renueva el token JWT |

> Los endpoints de scrapers y gesti√≥n requieren autenticaci√≥n mediante **Bearer Token JWT** en el header `Authorization`.

### Ejemplo de uso

```bash
# Obtener token
curl -X POST http://localhost:8000/auth/login \
  -H "Content-Type: application/json" \
  -d '{"username": "admin", "password": "tu_password"}'

# Consultar precios de leche
curl http://localhost:8000/productos?nombre=leche \
  -H "Authorization: Bearer <tu_token>"

# Disparar scraper de Carulla
curl -X POST http://localhost:8000/scrapers/run/carulla \
  -H "Authorization: Bearer <tu_token>"
```

---

## üê≥ Docker

El proyecto incluye un `Dockerfile` y `docker-compose.yml` para levantar todos los servicios necesarios:

```bash
# Construir y levantar todos los servicios
docker-compose up --build

# Ejecutar en segundo plano
docker-compose up -d

# Ver logs
docker-compose logs -f

# Detener servicios
docker-compose down
```

---

## üìÅ Variables de Entorno

El archivo `.env` debe contener las siguientes variables (ajusta los valores seg√∫n tu entorno):

```env
# Base de datos
DB_HOST=localhost
DB_PORT=5432
DB_NAME=supermercados
DB_USER=your_user
DB_PASSWORD=your_password

# Celery / Redis
CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_BACKEND=redis://localhost:6379/0
```

> ‚ö†Ô∏è **Nunca subas el archivo `.env` con credenciales reales al repositorio.**

---

## ü§ù Contribuciones

Las contribuciones son bienvenidas. Por favor sigue estos pasos:

1. Haz un fork del repositorio
2. Crea una rama para tu feature: `git checkout -b feature/nombre-feature`
3. Realiza tus cambios y haz commit: `git commit -m "feat: descripci√≥n del cambio"`
4. Sube tu rama: `git push origin feature/nombre-feature`
5. Abre un Pull Request

---

## üìÑ Licencia

Este proyecto est√° bajo la licencia MIT. Consulta el archivo `LICENSE` para m√°s detalles.

---

## üë§ Autor

**Hardostascon**  
[GitHub](https://github.com/hardostascon)
